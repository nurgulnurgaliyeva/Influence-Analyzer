{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. Extracting data from JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and CSV file creation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def extract_info(data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    owner = data.get('owner', {})\n",
    "    user = f\"{owner.get('full_name', '')} (@{owner.get('username', '')})\"\n",
    "    location = data.get('location', {})\n",
    "    location_name = location.get('name', '') if location else 'N/A'\n",
    "    \n",
    "    likes = data.get('edge_media_preview_like', {}).get('count', 0)\n",
    "    comments_data = data.get('edge_media_to_parent_comment', {})\n",
    "    comments_count = comments_data.get('count', 0)\n",
    "    comments_list = ' | '.join(comment['node']['text'] for comment in comments_data.get('edges', []))\n",
    "    \n",
    "    tagged_brands = ', '.join(user['node']['user']['full_name'] for user in data.get('edge_media_to_tagged_user', {}).get('edges', []))\n",
    "    \n",
    "    captions = data.get('edge_media_to_caption', {}).get('edges', [])\n",
    "    return [{\n",
    "        'User': user,\n",
    "        'Location': location_name,\n",
    "        'Caption Text': caption['node']['text'],\n",
    "        'Tags': ' '.join(tag for tag in caption['node']['text'].split() if tag.startswith('#')),\n",
    "        'Likes': likes,\n",
    "        'Comments Count': comments_count,\n",
    "        'Comments List': comments_list,\n",
    "        'Tagged Brands': tagged_brands\n",
    "    } for caption in captions]\n",
    "\n",
    "def process_files(input_path: str) -> List[Dict[str, Any]]:\n",
    "    data_list = []\n",
    "    for file_name in os.listdir(input_path):\n",
    "        if file_name.endswith('.info'):\n",
    "            with open(os.path.join(input_path, file_name), 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            data_list.extend(extract_info(json_data))\n",
    "    return data_list\n",
    "\n",
    "def write_csv(output_csv: str, data_list: List[Dict[str, Any]]) -> None:\n",
    "    fieldnames = ['User', 'Location', 'Caption Text', 'Tags', 'Likes', 'Comments Count', 'Comments List', 'Tagged Brands']\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data_list)\n",
    "\n",
    "def main():\n",
    "    input_path = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/info\"\n",
    "    output_csv = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/1. extracted_data.csv\"\n",
    "    \n",
    "    data_list = process_files(input_path)\n",
    "    write_csv(output_csv, data_list)\n",
    "    print(\"Data extraction and CSV file creation completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2. Preprocessing** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Caption Text' column processed and renamed to 'Cleaned Captions'\n",
      "'Comments List' column processed and renamed to 'Cleaned Comments'\n",
      "'User' column processed and renamed to 'User'\n",
      "Data preprocessing completed successfully.\n",
      "Final columns: Index(['User', 'Location', 'Cleaned Captions', 'Likes', 'Tags',\n",
      "       'Comments Count', 'Tagged Brands', 'Cleaned Comments'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from typing import Union, Callable\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Define file locations\n",
    "INPUT_FILE = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/1. extracted_data.csv\"\n",
    "OUTPUT_FILE = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/2. preprocessing_data.csv\"\n",
    "DESIRED_COLUMN_ORDER = ['User', 'Location', 'Cleaned Captions', 'Likes', 'Comments count', 'Comments list', 'Tagged brands', 'Tags']\n",
    "\n",
    "# Compile regex patterns\n",
    "SPECIAL_CHAR_PATTERN = re.compile(r'[^A-Za-z0-9\\s|]')\n",
    "URL_PATTERN = re.compile(r'http\\S+|www\\S+|https\\S+', flags=re.MULTILINE)\n",
    "USERNAME_PATTERN = re.compile(r'\\((@[\\w\\._]+)\\)')\n",
    "\n",
    "# Get stop words\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text: Union[str, float]) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove URLs\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    # Remove special characters (except '|') and convert to lowercase\n",
    "    text = SPECIAL_CHAR_PATTERN.sub('', text.lower())\n",
    "    # Tokenize and remove stop words\n",
    "    return ' '.join(word for word in word_tokenize(text) if word not in STOP_WORDS)\n",
    "\n",
    "def clean_username(username: str) -> str:\n",
    "    match = USERNAME_PATTERN.search(username.strip().lower())\n",
    "    return match.group(1).replace('@', '').strip() if match else username\n",
    "\n",
    "def process_column(df: pd.DataFrame, old_col: str, new_col: str, process_func: Callable) -> pd.DataFrame:\n",
    "    if old_col in df.columns:\n",
    "        df[new_col] = df[old_col].apply(process_func)\n",
    "        print(f\"'{old_col}' column processed and renamed to '{new_col}'\")\n",
    "    else:\n",
    "        print(f\"'{old_col}' column not found in DataFrame\")\n",
    "    return df\n",
    "\n",
    "def reorder_columns(df: pd.DataFrame, desired_order: list) -> pd.DataFrame:\n",
    "    existing_columns = [col for col in desired_order if col in df.columns]\n",
    "    remaining_columns = [col for col in df.columns if col not in existing_columns]\n",
    "    return df[existing_columns + remaining_columns]\n",
    "\n",
    "def main():\n",
    "    # Load the CSV data\n",
    "    data = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # Clean column names\n",
    "    data.columns = data.columns.str.strip()\n",
    "    \n",
    "    # Clean the 'Caption Text' and 'Comments List' columns\n",
    "    data = process_column(data, 'Caption Text', 'Cleaned Captions', clean_text)\n",
    "    data = process_column(data, 'Comments List', 'Cleaned Comments', clean_text)\n",
    "    \n",
    "    # Clean the 'User' column\n",
    "    data = process_column(data, 'User', 'User', clean_username)\n",
    "    \n",
    "    # Handle missing values\n",
    "    data.dropna(subset=['Cleaned Captions'], inplace=True)\n",
    "    data.fillna('', inplace=True)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    data.drop(columns=['Caption Text', 'Comments List'], inplace=True)\n",
    "    \n",
    "    # Reorder columns\n",
    "    data = reorder_columns(data, DESIRED_COLUMN_ORDER)\n",
    "    \n",
    "    # Save the cleaned data to a new CSV file\n",
    "    data.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(\"Data preprocessing completed successfully.\")\n",
    "    print(\"Final columns:\", data.columns)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3. Merging cleaned data with categories and number of followers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Username Category  Followers  Followees  Posts\n",
      "0        makeupbynvs   beauty       1432       1089    363\n",
      "1  jaquelinevandoski   beauty     137600        548    569\n",
      "           User Location                                   Cleaned Captions  \\\n",
      "0   lelien_tomo      NaN  danielwellington 75 75729 cuff 50off offtomo20...   \n",
      "1  veverkakokos      NaN  dneska tohle pocasi zase bylo co vic chtit sun...   \n",
      "\n",
      "   Likes                                               Tags  Comments Count  \\\n",
      "0    321  #danielwellington #ダニエルウェリントン #myclassicdw #サマ...              16   \n",
      "1     33  #sunnyday #photography #nature #pond #energy #...               0   \n",
      "\n",
      "       Tagged Brands        Cleaned Comments  \n",
      "0  Daniel Wellington  | | | tomotomo | | w |  \n",
      "1                NaN                     NaN  \n",
      "Sample rows from merged_df with followers and category:\n",
      "            User     Location  \\\n",
      "0    lelien_tomo          NaN   \n",
      "1   veverkakokos          NaN   \n",
      "2     alicekings          NaN   \n",
      "3  nicolenic1973       Ostsee   \n",
      "4        aleyahs  White Point   \n",
      "\n",
      "                                    Cleaned Captions  Likes  \\\n",
      "0  danielwellington 75 75729 cuff 50off offtomo20...    321   \n",
      "1  dneska tohle pocasi zase bylo co vic chtit sun...     33   \n",
      "2                                              quack     69   \n",
      "3  im trben november brauche ich stimmungsaufhell...    632   \n",
      "4  island attire inspired alex colville photo rac...     78   \n",
      "\n",
      "                                                Tags  Comments Count  \\\n",
      "0  #danielwellington #ダニエルウェリントン #myclassicdw #サマ...              16   \n",
      "1  #sunnyday #photography #nature #pond #energy #...               0   \n",
      "2                                                NaN               2   \n",
      "3  #Meer #meerliebe #vitaminsea #ostsee #ocean #l...              12   \n",
      "4  #capebretonisland #whitepoint #newhaven #visit...              10   \n",
      "\n",
      "                                       Tagged Brands  \\\n",
      "0                                  Daniel Wellington   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4  Rachel Solomon, Nova Scotia, Nikon France 🇫🇷, ...   \n",
      "\n",
      "                                    Cleaned Comments       Username  \\\n",
      "0                             | | | tomotomo | | w |    lelien_tomo   \n",
      "1                                                NaN   veverkakokos   \n",
      "2                                                fin     alicekings   \n",
      "3  viele wundervolle farben und ganz viel sonne i...  nicolenic1973   \n",
      "4  perfect | great shot | love | good one | loooo...        aleyahs   \n",
      "\n",
      "   Followers  Category  \n",
      "0     8836.0  interior  \n",
      "1     5223.0      food  \n",
      "2     4049.0   fashion  \n",
      "3     2781.0    travel  \n",
      "4     2063.0    travel  \n",
      "Followers and Category added to the cleaned Instagram data successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file locations\n",
    "FOLLOWERS_FILE = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/Number of followers for each influencer.csv\"\n",
    "CLEANED_DATA_FILE = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/2. preprocessing_data.csv\"\n",
    "OUTPUT_FILE = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/3. integrate_category_and_followers.csv\"\n",
    "\n",
    "def load_and_clean_df(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "def merge_dataframes(cleaned_data_df: pd.DataFrame, followers_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cleaned_data_df['User'] = cleaned_data_df['User'].str.strip().str.lower()\n",
    "    cleaned_data_df['User'] = cleaned_data_df['User'].str.replace('@', '').str.strip()\n",
    "    \n",
    "    # Strip leading/trailing spaces in usernames\n",
    "    followers_df['Username'] = followers_df['Username'].str.strip()\n",
    "    cleaned_data_df['User'] = cleaned_data_df['User'].str.strip()\n",
    "\n",
    "    # Convert usernames to lower case to avoid case sensitivity issues\n",
    "    followers_df['Username'] = followers_df['Username'].str.lower()\n",
    "    cleaned_data_df['User'] = cleaned_data_df['User'].str.lower()\n",
    "    \n",
    "    print(followers_df.head(2))\n",
    "    print(cleaned_data_df.head(2))\n",
    "\n",
    "    # Merge the two dataframes on the username column, including the Category column\n",
    "    merged_df = pd.merge(cleaned_data_df, followers_df[['Username', 'Followers', 'Category']], left_on='User', right_on='Username', how='left')\n",
    "\n",
    "    # Check if the merge was successful by printing some rows with followers and category\n",
    "    print(\"Sample rows from merged_df with followers and category:\")\n",
    "    print(merged_df[~merged_df['Followers'].isna()].head())\n",
    "    return merged_df \n",
    "\n",
    "def main():\n",
    "    # Load and clean dataframes\n",
    "    followers_df = load_and_clean_df(FOLLOWERS_FILE)\n",
    "    cleaned_data_df = load_and_clean_df(CLEANED_DATA_FILE)\n",
    "\n",
    "    # Merge dataframes\n",
    "    merged_df = merge_dataframes(cleaned_data_df, followers_df)\n",
    "\n",
    "    # Save merged dataframe\n",
    "    merged_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(\"Followers and Category added to the cleaned Instagram data successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4. Removing non-english words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.2 (v3.12.2:6abddd9f6a, Feb  6 2024, 17:02:06) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "spaCy version: 3.7.5\n",
      "Path to spaCy: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/__init__.py\n",
      "\n",
      "Installed models:\n",
      " - en_core_web_sm\n",
      "\n",
      "Attempting to load 'en_core_web_sm':\n",
      " - Model loaded successfully\n",
      "                          User     Location  \\\n",
      "2                   alicekings          NaN   \n",
      "4                      aleyahs  White Point   \n",
      "6                   merakilane          NaN   \n",
      "7                 zanabfarooq_          NaN   \n",
      "8             lightscamerabake          NaN   \n",
      "...                        ...          ...   \n",
      "39540  caroliinaneerotofficial  Dover, Kent   \n",
      "39541       practicalbydefault          NaN   \n",
      "39543                 power953          NaN   \n",
      "39544                    q1043          NaN   \n",
      "39545                prekpages          NaN   \n",
      "\n",
      "                                        Cleaned Captions  Likes  \\\n",
      "2                                                  quack     69   \n",
      "4      island attire inspired alex colville photo rac...     78   \n",
      "6      one friday afternoon rituals sit write list go...    198   \n",
      "7                                       tis season jolly    568   \n",
      "8      new blog today peanut butter bars link bio recipe     78   \n",
      "...                                                  ...    ...   \n",
      "39540  dover beach vibes pic hubby instagramhusband s...     71   \n",
      "39541  theres truth busy moms work homeschool many as...     37   \n",
      "39543                                       ladies sound     21   \n",
      "39544  whos excited openingday today yankees home met...    157   \n",
      "39545  looking fun yet easy preschool science experim...     99   \n",
      "\n",
      "                                                    Tags  Comments Count  \\\n",
      "2                                                    NaN               2   \n",
      "4      #capebretonisland #whitepoint #newhaven #visit...              10   \n",
      "6                                                    NaN               8   \n",
      "7                                                    NaN              17   \n",
      "8                                          #blog #recipe               7   \n",
      "...                                                  ...             ...   \n",
      "39540  #Instagramhusband #liketkit #LTKfit #LTKcurves...              10   \n",
      "39541  #workinghomeschoolmom #workingmom #workingmoml...               3   \n",
      "39543                                                NaN              28   \n",
      "39544                                        #OpeningDay               5   \n",
      "39545  #preschool #preschool #teachingtribe #theteach...               1   \n",
      "\n",
      "                                           Tagged Brands  \\\n",
      "2                                                    NaN   \n",
      "4      Rachel Solomon, Nova Scotia, Nikon France 🇫🇷, ...   \n",
      "6                                                    NaN   \n",
      "7                                                    NaN   \n",
      "8                                                    NaN   \n",
      "...                                                  ...   \n",
      "39540  , QUAY AUSTRALIA, Bondi Sands, LIKEtoKNOW.it, ...   \n",
      "39541                                                NaN   \n",
      "39543                                                NaN   \n",
      "39544                                                NaN   \n",
      "39545                                                NaN   \n",
      "\n",
      "                                        Cleaned Comments  \\\n",
      "2                                                    fin   \n",
      "4      perfect | great shot | love | good one | loooo...   \n",
      "6      merakilane lifeisgood behappy thatsdarling lif...   \n",
      "7      omg love whole christmasy vibes | fun seeing d...   \n",
      "8      lightscamerabake foodporn foodofinsta instafoo...   \n",
      "...                                                  ...   \n",
      "39540  loving entire looklets get featured merchandis...   \n",
      "39541  yes | truth yes taking little time develop sys...   \n",
      "39543  flowers kobes | coffee | | flowers candy rhern...   \n",
      "39544  go yankeeslets go metsawsomest best best djs10...   \n",
      "39545              vanessa thanks sharing great ideas us   \n",
      "\n",
      "                      Username  Followers Category Language  \n",
      "2                   alicekings     4049.0  fashion       en  \n",
      "4                      aleyahs     2063.0   travel       en  \n",
      "6                   merakilane    15464.0     food       en  \n",
      "7                 zanabfarooq_    17543.0  fashion       en  \n",
      "8             lightscamerabake     1361.0     food       en  \n",
      "...                        ...        ...      ...      ...  \n",
      "39540  caroliinaneerotofficial     1403.0  fashion       en  \n",
      "39541       practicalbydefault     1649.0    other       en  \n",
      "39543                 power953    25536.0    other       en  \n",
      "39544                    q1043     5250.0    other       en  \n",
      "39545                prekpages    30944.0    other       en  \n",
      "\n",
      "[23168 rows x 12 columns]\n",
      "Non-English captions and comments removed successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import pandas as pd\n",
    "\n",
    "# Diagnostic information\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"spaCy version: {spacy.__version__}\")\n",
    "print(f\"Path to spaCy: {spacy.__file__}\")\n",
    "\n",
    "print(\"\\nInstalled models:\")\n",
    "for model in spacy.util.get_installed_models():\n",
    "    print(f\" - {model}\")\n",
    "\n",
    "print(\"\\nAttempting to load 'en_core_web_sm':\")\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\" - Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\" - Error loading model: {e}\")\n",
    "    print(\"Attempting to download 'en_core_web_sm'...\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a factory function to create the LanguageDetector if it does not already exist\n",
    "if not Language.has_factory(\"language_detector\"):\n",
    "    @Language.factory(\"language_detector\")\n",
    "    def create_language_detector(nlp, name):\n",
    "        return LanguageDetector()\n",
    "\n",
    "# Add the LanguageDetector component to the pipeline if it's not already there\n",
    "if \"language_detector\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"language_detector\", last=True)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/Users/nurgul/Documents/Projects/Dissertation Code/data/3. integrate_category_and_followers.csv')\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        doc = nlp(text)\n",
    "        return doc._.language['language']\n",
    "    else:\n",
    "        return 'unknown'  # Handle non-string inputs\n",
    "\n",
    "# Apply the function to the DataFrame and create a new column\n",
    "df['Language'] = df['Cleaned Comments'].apply(detect_language)\n",
    "\n",
    "# Filter the DataFrame to keep only English rows\n",
    "df_english = df[df['Language'] == 'en']\n",
    "\n",
    "# Save the filtered DataFrame back to the file\n",
    "df_english.to_csv('/Users/nurgul/Documents/Projects/Dissertation Code/data/4. english_captions.csv', index=False)\n",
    "\n",
    "print(df_english)\n",
    "\n",
    "# Print success message\n",
    "print(\"Non-English captions and comments removed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5. Calculating number of influencers per each category.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of influencers in each category:\n",
      "   Category  Number of Influencers\n",
      "0    beauty                    637\n",
      "1    family                   1824\n",
      "2   fashion                   5338\n",
      "3    fasion                      1\n",
      "4   fitness                    547\n",
      "5      food                   1730\n",
      "6  interior                    566\n",
      "7     other                   2103\n",
      "8       pet                    304\n",
      "9    travel                   1881\n",
      "Data saved successfully to /Users/nurgul/Documents/Projects/Dissertation Code/data/5. influencers_per_category.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define file locations\n",
    "INPUT_FILE = Path(\"/Users/nurgul/Documents/Projects/Dissertation Code/data/4. english_captions.csv\")\n",
    "OUTPUT_FILE = Path(\"/Users/nurgul/Documents/Projects/Dissertation Code/data/5. influencers_per_category.csv\")\n",
    "\n",
    "def load_data(file_path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def calculate_influencers_per_category(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result = df.groupby('Category')['Username'].nunique().reset_index()\n",
    "    result.columns = ['Category', 'Number of Influencers']\n",
    "    return result\n",
    "\n",
    "def save_data(df: pd.DataFrame, file_path: Path) -> None:\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Data saved successfully to {file_path}\")\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    merged_df = load_data(INPUT_FILE)\n",
    "\n",
    "    # Calculate influencers per category\n",
    "    influencers_per_category = calculate_influencers_per_category(merged_df)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Number of influencers in each category:\")\n",
    "    print(influencers_per_category)\n",
    "\n",
    "    # Save results\n",
    "    save_data(influencers_per_category, OUTPUT_FILE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Step 6. Remaining only necessary categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved successfully to /Users/nurgul/Documents/Projects/Dissertation Code/data/6. cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Define file locations\n",
    "INPUT_FILE = Path(\"/Users/nurgul/Documents/Projects/Dissertation Code/data/4. english_captions.csv\")\n",
    "OUTPUT_FILE = Path(\"/Users/nurgul/Documents/Projects/Dissertation Code/data/6. cleaned_data.csv\")\n",
    "CATEGORIES_TO_KEEP = ['beauty', 'family', 'fashion', 'fitness', 'food', 'travel']\n",
    "\n",
    "def load_data(file_path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def filter_categories(df: pd.DataFrame, categories: List[str]) -> pd.DataFrame:\n",
    "    return df[df['Category'].isin(categories)]\n",
    "\n",
    "def save_data(df: pd.DataFrame, file_path: Path) -> None:\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Filtered data saved successfully to {file_path}\")\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    merged_df = load_data(INPUT_FILE)\n",
    "\n",
    "    # Filter categories\n",
    "    filtered_df = filter_categories(merged_df, CATEGORIES_TO_KEEP)\n",
    "\n",
    "    # Save filtered data\n",
    "    save_data(filtered_df, OUTPUT_FILE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Step 7. Sentiment and keywords analysis - positive only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /Users/nurgul/Documents/Projects/Dissertation Code/data/7. sentiment_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qk/fv3tjmnj2vl20mq7xbr_rk6w0000gn/T/ipykernel_7504/3422327630.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['top_keywords'] = df['Cleaned Captions'].apply(lambda x: get_top_keywords_predefined(x, ALL_KEYWORDS))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Distribution:\n",
      "categories\n",
      "fashion    0.263747\n",
      "family     0.224729\n",
      "travel     0.149235\n",
      "food       0.139556\n",
      "beauty     0.124070\n",
      "fitness    0.098663\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample Top Keywords:\n",
      "1    [vacation, trip, adventure, destination, explore]\n",
      "4                                             [recipe]\n",
      "6                                   [fashion, couture]\n",
      "8                                            [explore]\n",
      "9                          [dress, health, food, chef]\n",
      "Name: top_keywords, dtype: object\n",
      "Data saved to /Users/nurgul/Documents/Projects/Dissertation Code/data/7.1 keywords_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Callable\n",
    "\n",
    "# Define file locations and names\n",
    "DATA_DIR = Path(\"/Users/nurgul/Documents/Projects/Dissertation Code/data\")\n",
    "INPUT_FILE = \"6. cleaned_data.csv\"\n",
    "POSITIVE_OUTPUT_FILE = \"7. sentiment_analysis.csv\"\n",
    "FINAL_OUTPUT_FILE = \"7.1 keywords_analysis.csv\"\n",
    "\n",
    "CATEGORY_KEYWORDS = {\n",
    "    'beauty': ['makeup', 'skincare', 'beauty', 'cosmetics', 'hair', 'nails', 'facial', 'lipstick', 'eyeliner', 'mascara',\n",
    "               'foundation', 'concealer', 'blush', 'bronzer', 'highlighter', 'eyeshadow', 'serum', 'moisturizer', 'cleanser', 'toner'],\n",
    "    'fashion': ['style', 'outfit', 'fashion', 'clothes', 'accessories', 'dress', 'shoes', 'handbag', 'jewelry', 'trend',\n",
    "                'designer', 'runway', 'vintage', 'streetwear', 'couture', 'boutique', 'sustainable', 'chic', 'glamour', 'wardrobe'],\n",
    "    'family': ['family', 'kids', 'parenting', 'children', 'home', 'baby', 'mom', 'dad', 'sibling', 'grandparent',\n",
    "               'adoption', 'education', 'teenager', 'toddler', 'childcare', 'family-planning', 'homeschooling', 'co-parenting', 'stepfamily', 'foster'],\n",
    "    'fitness': ['workout', 'gym', 'fitness', 'exercise', 'health', 'muscle', 'training', 'cardio', 'strength', 'yoga',\n",
    "                'pilates', 'crossfit', 'weightlifting', 'running', 'cycling', 'meditation', 'nutrition', 'flexibility', 'HIIT', 'bodyweight'],\n",
    "    'food': ['recipe', 'cooking', 'food', 'meal', 'restaurant', 'cuisine', 'diet', 'nutrition', 'chef', 'baking',\n",
    "             'vegan', 'organic', 'gluten-free', 'barbecue', 'farm-to-table', 'meal-prep', 'slow-cooker', 'fusion', 'foodie', 'paleo'],\n",
    "    'travel': ['travel', 'vacation', 'trip', 'adventure', 'destination', 'tourism', 'hotel', 'flight', 'explore', 'sightseeing',\n",
    "               'backpacking', 'ecotourism', 'cruise', 'resort', 'cultural-exchange', 'roadtrip', 'staycation', 'digital-nomad', 'budget-travel', 'luxury-travel']\n",
    "}\n",
    "\n",
    "ALL_KEYWORDS = [word for words in CATEGORY_KEYWORDS.values() for word in words]\n",
    "\n",
    "\n",
    "def setup_nltk():\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "def load_data(file_path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def analyze_sentiment(text: str) -> Dict[str, float or str]:\n",
    "    if pd.isna(text):\n",
    "        return {'score': 0, 'category': 'Neutral'}\n",
    "    sentiment_score = SentimentIntensityAnalyzer().polarity_scores(text)['compound']\n",
    "    category = 'Positive' if sentiment_score > 0.05 else 'Negative' if sentiment_score < -0.05 else 'Neutral'\n",
    "    return {'score': sentiment_score, 'category': category}\n",
    "\n",
    "def apply_sentiment_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sentiment_results = df['Cleaned Comments'].apply(analyze_sentiment)\n",
    "    df['Sentiment Score'] = sentiment_results.apply(lambda x: x['score'])\n",
    "    df['Sentiment Category'] = sentiment_results.apply(lambda x: x['category'])\n",
    "    return df\n",
    "\n",
    "def categorize_post(text: str) -> List[str]:\n",
    "    text = str(text).lower()\n",
    "    return [category for category, keywords in CATEGORY_KEYWORDS.items() if any(keyword in text for keyword in keywords)]\n",
    "\n",
    "def get_top_keywords_predefined(text: str, keywords: List[str], top_n: int = 5) -> List[str]:\n",
    "    text = str(text).lower()\n",
    "    return [word for word in keywords if word in text][:top_n]\n",
    "\n",
    "def get_top_keywords_tfidf(tfidf_vector, feature_names: List[str], top_n: int = 5) -> List[str]:\n",
    "    sorted_items = sorted(zip(tfidf_vector.tocoo().col, tfidf_vector.tocoo().data), key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    return [feature_names[idx] for idx, _ in sorted_items[:top_n]]\n",
    "\n",
    "def apply_keyword_analysis(df: pd.DataFrame, method: str = 'predefined') -> pd.DataFrame:\n",
    "    df['categories'] = df['Cleaned Captions'].apply(categorize_post)\n",
    "    df = df[df['categories'].apply(len) > 0]\n",
    "    \n",
    "    if method == 'predefined':\n",
    "        df['top_keywords'] = df['Cleaned Captions'].apply(lambda x: get_top_keywords_predefined(x, ALL_KEYWORDS))\n",
    "    else:\n",
    "        tfidf = TfidfVectorizer(max_features=1000)\n",
    "        tfidf_matrix = tfidf.fit_transform(df['Cleaned Captions'].fillna(''))\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        df['top_keywords'] = [get_top_keywords_tfidf(tfidf_matrix[i], feature_names) for i in range(tfidf_matrix.shape[0])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_results(df: pd.DataFrame):\n",
    "    print(\"Category Distribution:\")\n",
    "    print(df['categories'].explode().value_counts(normalize=True))\n",
    "    print(\"\\nSample Top Keywords:\")\n",
    "    print(df['top_keywords'].head())\n",
    "\n",
    "def save_data(df: pd.DataFrame, file_path: Path):\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Data saved to {file_path}\")\n",
    "\n",
    "def main():\n",
    "    setup_nltk()\n",
    "    \n",
    "    df = load_data(DATA_DIR / INPUT_FILE)\n",
    "    df_sentiment = apply_sentiment_analysis(df)\n",
    "    save_data(df_sentiment, DATA_DIR / POSITIVE_OUTPUT_FILE)\n",
    "    \n",
    "    df_categorized = apply_keyword_analysis(df_sentiment)\n",
    "    print_results(df_categorized)\n",
    "    save_data(df_categorized, DATA_DIR / FINAL_OUTPUT_FILE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 8. SEMANTIC ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (4.43.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Added batch of 500 documents. Total added: 500\n",
      "Added batch of 500 documents. Total added: 1000\n",
      "Added batch of 500 documents. Total added: 1500\n",
      "Added batch of 500 documents. Total added: 2000\n",
      "Added batch of 500 documents. Total added: 2500\n",
      "Added batch of 500 documents. Total added: 3000\n",
      "Added batch of 500 documents. Total added: 3500\n",
      "Added batch of 500 documents. Total added: 4000\n",
      "Added batch of 500 documents. Total added: 4500\n",
      "Added batch of 500 documents. Total added: 5000\n",
      "Added batch of 500 documents. Total added: 5500\n",
      "Added batch of 500 documents. Total added: 6000\n",
      "Added batch of 500 documents. Total added: 6500\n",
      "Added batch of 500 documents. Total added: 7000\n",
      "Added batch of 500 documents. Total added: 7500\n",
      "Added batch of 500 documents. Total added: 8000\n",
      "Added batch of 500 documents. Total added: 8500\n",
      "Added batch of 500 documents. Total added: 9000\n",
      "Added batch of 500 documents. Total added: 9500\n",
      "Added batch of 500 documents. Total added: 10000\n",
      "Added batch of 500 documents. Total added: 10500\n",
      "Added batch of 500 documents. Total added: 11000\n",
      "Added batch of 500 documents. Total added: 11500\n",
      "Added batch of 500 documents. Total added: 12000\n",
      "Added batch of 500 documents. Total added: 12500\n",
      "Added batch of 500 documents. Total added: 13000\n",
      "Added batch of 500 documents. Total added: 13500\n",
      "Added batch of 500 documents. Total added: 14000\n",
      "Added batch of 500 documents. Total added: 14500\n",
      "Added batch of 500 documents. Total added: 15000\n",
      "Added batch of 500 documents. Total added: 15500\n",
      "Added batch of 500 documents. Total added: 16000\n",
      "Added batch of 500 documents. Total added: 16500\n",
      "Added batch of 500 documents. Total added: 17000\n",
      "Added batch of 500 documents. Total added: 17500\n",
      "Added batch of 500 documents. Total added: 18000\n",
      "Added batch of 403 documents. Total added: 18403\n",
      "Finished adding 18403 documents to the collection.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers\n",
    "import csv\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import re\n",
    "\n",
    "# Function to clean and convert number strings\n",
    "def clean_number(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove any non-digit characters (except decimal point)\n",
    "        clean_value = re.sub(r'[^\\d.]', '', value)\n",
    "        try:\n",
    "            return int(float(clean_value))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return int(value)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define file locations\n",
    "file_path = '/Users/nurgul/Documents/Projects/Dissertation Code/data/7. sentiment_analysis.csv'\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Get or create a collection\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"sentiment_analysis\", \n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Function to delete documents in batches\n",
    "def delete_in_batches(collection, batch_size=5000):\n",
    "    while True:\n",
    "        results = collection.get(limit=batch_size)\n",
    "        if not results['ids']:\n",
    "            break\n",
    "        collection.delete(ids=results['ids'])\n",
    "        print(f\"Deleted batch of {len(results['ids'])} documents.\")\n",
    "\n",
    "# Check if the collection already has data\n",
    "if collection.count() > 0:\n",
    "    user_input = input(\"The collection already contains data. Do you want to clear it and repopulate? (yes/no): \")\n",
    "    if user_input.lower() == 'yes':\n",
    "        print(\"Clearing existing data...\")\n",
    "        delete_in_batches(collection)\n",
    "        print(\"Collection cleared. Repopulating with new data.\")\n",
    "    else:\n",
    "        print(\"Keeping existing data. Script will now exit.\")\n",
    "        exit()\n",
    "\n",
    "# Load data\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    \n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    \n",
    "    for i, row in enumerate(csv_reader):\n",
    "        # Combine Cleaned Caption and Cleaned Comments for document content\n",
    "        document_content = f\"Captions: {row['Cleaned Captions']} | Comments: {row['Cleaned Comments']}\"\n",
    "        documents.append(document_content)\n",
    "        \n",
    "        metadatas.append({\n",
    "            \"user\": row['User'],\n",
    "            \"location\": row['Location'],\n",
    "            \"tags\": row['Tags'],\n",
    "            \"likes\": clean_number(row['Likes']),\n",
    "            \"comments_count\": clean_number(row['Comments Count']),\n",
    "            \"tagged_brands\": row['Tagged Brands'],\n",
    "            \"username\": row['Username'],\n",
    "            \"followers\": clean_number(row['Followers']),\n",
    "            \"category\": row['Category'],\n",
    "            \"language\": row['Language'],\n",
    "            \"sentiment_score\": float(row['Sentiment Score']) if row['Sentiment Score'] else 0.0,\n",
    "            \"sentiment_category\": row['Sentiment Category'],\n",
    "            \"cleaned_captions\": row['Cleaned Captions'],\n",
    "            \"cleaned_comments\": row['Cleaned Comments']\n",
    "        })\n",
    "        ids.append(str(i))\n",
    "\n",
    "# Add data to the vector database in batches\n",
    "batch_size = 500  \n",
    "total_added = 0\n",
    "\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch_documents = documents[i:i+batch_size]\n",
    "    batch_metadatas = metadatas[i:i+batch_size]\n",
    "    batch_ids = ids[i:i+batch_size]\n",
    "\n",
    "    try:\n",
    "        collection.add(\n",
    "            documents=batch_documents,\n",
    "            metadatas=batch_metadatas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "        total_added += len(batch_documents)\n",
    "        print(f\"Added batch of {len(batch_documents)} documents. Total added: {total_added}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding batch: {e}\")\n",
    "\n",
    "print(f\"Finished adding {total_added} documents to the collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9. Query Setup and Result Formatting for Sentiment Analysis Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "# Get the existing collection\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "collection = client.get_collection(name=\"sentiment_analysis\", embedding_function=sentence_transformer_ef)\n",
    "def print_results(query, results):\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Number of results: {len(results['documents'][0])}\")\n",
    "    for doc, metadata, distance in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Distance: {distance}\")\n",
    "        print(f\"Caption: {metadata['cleaned_captions']}\")\n",
    "        print(f\"Comments: {metadata['cleaned_comments']}\")\n",
    "        print(f\"User: {metadata['user']}\")\n",
    "        print(f\"Location: {metadata['location']}\")\n",
    "        print(f\"Tags: {metadata['tags']}\")\n",
    "        print(f\"Category: {metadata['category']}\")\n",
    "        print(f\"Sentiment Category: {metadata['sentiment_category']}\")\n",
    "        print(f\"Sentiment Score: {metadata['sentiment_score']}\")\n",
    "        print(f\"Likes: {metadata['likes']}\")\n",
    "        print(f\"Followers: {metadata['followers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 10. Executing and Displaying Targeted Query for Travel and Food Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'I'm looking for travel influencers who shares diverse cuisines from around the world with high engagement rate.'\n",
      "Number of results: 5\n",
      "--------------------------------------------------\n",
      "Distance: 1.1496665477752686\n",
      "Caption: soul food sessions mission increase diversity culinary industry partnering cocacolaconsolidated bring popup dinners titled table set multiple cities tour wrapping thedewberrycharleston october 16th chef greg collier contributing featured dish new orleans grillades green strawberry relish paired cocacola happy give try ingredients harris teeter wont last time find recipe buy tickets via link bio swiping stories soulfoodsessionsclt soulfoodsessions yourseatiswaiting thetableisset sponsored\n",
      "Comments: looks great sals delicious | wow looks delicious great initiative | looks awesome | excellent idea looks | fun cool idea | ill check | oh looks yummy | thats serious plate | earrings | looks fun ive seeing strawberry relish lately looks delicious | looks fantastic popping | strawberry relish | liked | flavors together | sounds delish fun | got ta love glass coke bottle | looks delish check | looks fantatstic | yum | nice | coke smile wasnt ad campaign back | coke | pass beef gim dat coke veggies | meal looks good\n",
      "User: blushingalpacas\n",
      "Location: Charleston, South Carolina\n",
      "Tags: #soulfoodsessions #yourseatiswaiting #thetableisset #sponsored\n",
      "Category: fashion\n",
      "Sentiment Category: Positive\n",
      "Sentiment Score: 0.9962\n",
      "Likes: 342\n",
      "Followers: 29041\n",
      "--------------------------------------------------\n",
      "Distance: 1.157527208328247\n",
      "Caption: know la matta offering happy hour specials 47 every day tap wines featured beers 25 course theres plenty food choices charcuterie anyone foodie austinblogger lamattaaustin\n",
      "Comments: food foodporn momentsofchic inspocafe blogger forkyeah foodgasm austinblogger atx texas foodlover fashionblogger lifestyle weekendmoments todaysdetails foodie foodieblogger styleblogger fashionblogger texasblogger styleoftheday fashionblog thatsdarling casualoutfit bloggerstyle bloggerlife | favorite meal | | go | 1 n | nice charcuterie | wmhill taylormariewolfe go asap | | thats kind spread\n",
      "User: nicolesometimes\n",
      "Location: La Matta\n",
      "Tags: #charcuterie #foodie #austinblogger #lamattaaustin\n",
      "Category: fashion\n",
      "Sentiment Category: Positive\n",
      "Sentiment Score: 0.8481\n",
      "Likes: 212\n",
      "Followers: 14934\n",
      "--------------------------------------------------\n",
      "Distance: 1.1700897216796875\n",
      "Caption: reminiscing spread establishmentchs also beginning next week establishment host weekly wine tastings 500pm 630pm monday 15 guests sample curated selection five wines kick series next monday february 11 establishment waiving fee tasting tag someone needs free wineducation someone needs free wine\n",
      "Comments: tompkinsrose | thanks shoutout | | milena1185 | charlestonfoodiebabe tag tho | | ceal8ter | short list dinner next week thanks recommendation | htoribrooke glutenfreeandlovingit123 moday wine tastingsss | oolalal | janinesundin need go place ive hearing great things | twravenel know | lo15rn | company outingmeeting | katievryan tell john | kyliejennar rachelindsay | come want eat | dirtysoup760 | kbdbowens | kellymgerth | alluhdat rethmiriambarr lets go | lowcountryweenies | fresh wagyu menu yet check foodiecitynetwork distribution fresh wagyu beef air chilled chicken plant based seafood food service area lowcountry food brings people together especially delicious food fcn | foodcurated\n",
      "User: charlestonfoodiebabe\n",
      "Location: The Establishment CHS\n",
      "Tags: \n",
      "Category: food\n",
      "Sentiment Category: Positive\n",
      "Sentiment Score: 0.9571\n",
      "Likes: 420\n",
      "Followers: 17430\n",
      "--------------------------------------------------\n",
      "Distance: 1.170634388923645\n",
      "Caption: eyes foodart\n",
      "Comments: love | jymac | love | incredible art well done | | love | siskaa266 sellyli | woow | rodrigoserretti | msy14 | dont care ppl say true art | love | dang collage awesome | amazing | morgogata allimarieeee | wow jacobsfooddiaries | wow | ivounggulsetiawan buka galerinya deh keren keren bgt tp gak ada yg bisa ditiru wkwkk susah | ashleybreanne21 | steffiejo look food art | pags198 | good | kiraniaashaka22 follow ini kiran bgus2 | trudiford\n",
      "User: jacobs_food_diaries\n",
      "Location: \n",
      "Tags: #foodart\n",
      "Category: food\n",
      "Sentiment Category: Positive\n",
      "Sentiment Score: 0.9902\n",
      "Likes: 3518\n",
      "Followers: 128256\n",
      "--------------------------------------------------\n",
      "Distance: 1.199333667755127\n",
      "Caption: sunday funday brunch shady lady saloon top one top 25 bars america brunch selections inspired el salvadoran chef using handmade ingredients went infamous chilaquiles enjoyed spicy kick shadyladybar brunch sundayfunday chilaquiles foodie midtownsac sacramento sacramentofood brunch\n",
      "Comments: yum looks tasty mikekahn6 | shadylady shadyladysaloon brunchclub brunchlife brunching brunchfood foodie foodiegram sacramentoproud sacramentoca foodie foodiefix heartyfood heartymeal spicy spicyfood | aliciajacqueline727 sooooo good | love brunches | yum | kukla53 decently priced | mikekahn6 wo make home cooking | yes | yes | nice | soooo good | keep good work | holy yum | yes | | certo good | serve hot buttered rum thatsvto die | | | looks good\n",
      "User: realmklibrary\n",
      "Location: Shady Lady Bar\n",
      "Tags: #brunch #sundayfunday #chilaquiles #foodie #midtownsac #sacramento #sacramentofood #brunch🍴\n",
      "Category: travel\n",
      "Sentiment Category: Positive\n",
      "Sentiment Score: 0.9741\n",
      "Likes: 282\n",
      "Followers: 34614\n"
     ]
    }
   ],
   "source": [
    "# Query for travel influencers showcasing diverse cuisines\n",
    "query = \"I'm looking for travel influencers who shares diverse cuisines from around the world with high engagement rate.\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=5,\n",
    "    include=['documents', 'distances', 'metadatas']\n",
    ")\n",
    "print_results(query, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Popular food bloggers with kids'\n",
      "Number of results: 3\n",
      "--------------------------------------------------\n",
      "Distance: 0.838355541229248\n",
      "Caption: kids away parents play first stop breakfast whenthekidsareaway grownups bloggermom momlife momblog mommyblogger mommylife motherhood boymom toddlermom motherhoodthroughinstagram littleandbrave eternalmotherhood lovelysquares mytinymoments thehappynow honestmotherhood joyfulmamas ohheymama momlifestyle dailyparenting pursuepretty lifestyleblogger foodblogger azblogger recipeforasweetlife\n",
      "Comments: dont forget | cute check channel likeim poking fun better half might enjoy | cute sandals | gim leopard | enjoy | cute shoes | best one far\n",
      "User: recipeforasweetlife\n",
      "Location: \n",
      "Tags: #whenthekidsareaway #grownups #bloggermom #momlife #momblog #mommyblogger #mommylife #motherhood #boymom #toddlermom #motherhoodthroughinstagram #littleandbrave #eternalmotherhood #lovelysquares #mytinymoments #thehappynow #honestmotherhood #joyfulmamas #ohheymama #momlifestyle #dailyparenting #pursuepretty #lifestyleblogger #foodblogger #azblogger #recipeforasweetlife\n",
      "Category: food\n",
      "Sentiment Category: Positive\n",
      "Sentiment Score: 0.9006\n",
      "Likes: 86\n",
      "Followers: 1959\n",
      "--------------------------------------------------\n",
      "Distance: 0.880700945854187\n",
      "Caption: kids meals adorable habitburgergrill look cute candy mini burger included kids meal makeitahabit\n",
      "Comments: burger eat foodie food yum burger instafood travel delicious foodporn kidsmeal kidsmeals orlando orlandoblogger | super cute candy\n",
      "User: justmarlaz\n",
      "Location: The Habit Burger Grill\n",
      "Tags: #makeitahabit\n",
      "Category: food\n",
      "Sentiment Category: Positive\n",
      "Sentiment Score: 0.891\n",
      "Likes: 65\n",
      "Followers: 7783\n",
      "--------------------------------------------------\n",
      "Distance: 0.881286084651947\n",
      "Caption: moms classic spritz cookies recipe one need tiny cookies crunchy buttery perfect sprinkle colored sugar full recipe click link bio select photo culinaryhill\n",
      "Comments: thefeedfeed food52 tastespotting feedfeed buzzfeast f52grams tastespotting huffposttaste thekitchn forkyeah dailyfoodfeed beautifulcuisines foodwinewomen spoonfeed foodgawker freshfinds myopenkitchen rslove eatprettythings inmykitchen todayfood foodprnshare foodpornshare bhgfood gatheringslikethese foodbeast feedyoursoul darlingmovement darlingdaily yougottaeatthis christmasbaking christmas2017 sugarcookie | awesome | mygourmetcookbook thank | | mintulov2cook thanks | love picture makes miss momma\n",
      "User: culinaryhill\n",
      "Location: \n",
      "Tags: \n",
      "Category: food\n",
      "Sentiment Category: Positive\n",
      "Sentiment Score: 0.9201\n",
      "Likes: 195\n",
      "Followers: 37241\n"
     ]
    }
   ],
   "source": [
    "# Additional queries\n",
    "queries = [\n",
    "    \"Popular food bloggers with kids\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=3,\n",
    "        include=['documents', 'distances', 'metadatas']\n",
    "    )\n",
    "    print_results(query, results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 11. Exporting ChromaDB Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection exported successfully to 'sentiment_analysis_collection_export.json'\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import json\n",
    "\n",
    "def export_collection():\n",
    "    # Initialize ChromaDB client\n",
    "    client = chromadb.Client()\n",
    "\n",
    "    # Get the existing collection\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "    collection = client.get_collection(name=\"sentiment_analysis\", embedding_function=sentence_transformer_ef)\n",
    "\n",
    "    # Get all items from the collection\n",
    "    results = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "\n",
    "    # Prepare the data for export\n",
    "    export_data = {\n",
    "        'ids': results['ids'],\n",
    "        'embeddings': results['embeddings'],\n",
    "        'documents': results['documents'],\n",
    "        'metadatas': results['metadatas']\n",
    "    }\n",
    "\n",
    "    # Export to a JSON file\n",
    "    with open('sentiment_analysis_collection_export.json', 'w') as f:\n",
    "        json.dump(export_data, f)\n",
    "\n",
    "    print(\"Collection exported successfully to 'sentiment_analysis_collection_export.json'\")\n",
    "\n",
    "# Call the function to export the collection\n",
    "export_collection()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
