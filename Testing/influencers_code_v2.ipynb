{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging with followers data failed. Stopping execution.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect, LangDetectException\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# File paths\n",
    "BASE_PATH = os.path.expanduser(\"~/Documents/Projects/Dissertation Code/data\")\n",
    "RAW_DATA_PATH = os.path.expanduser(\"~/Library/CloudStorage/OneDrive-UniversityofEastLondon/DS7010_Dissertation/Data/info\")\n",
    "FOLLOWERS_FILE = os.path.join(BASE_PATH, \"Number of followers for each influencer.csv\")\n",
    "INTERIM_DATA_FILE = os.path.join(BASE_PATH, \"instagram_data.csv\")\n",
    "CLEANED_DATA_FILE = os.path.join(BASE_PATH, \"1.Cleaned_instagram_data.csv\")\n",
    "MERGED_DATA_FILE = os.path.join(BASE_PATH, \"2.Merged_instagram_data.csv\")\n",
    "FILTERED_DATA_FILE = os.path.join(BASE_PATH, \"3.Filtered_instagram_data.csv\")\n",
    "FINAL_DATA_FILE = os.path.join(BASE_PATH, \"4.Final_instagram_data.csv\")\n",
    "\n",
    "os.makedirs(BASE_PATH, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, float):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def extract_info(data):\n",
    "    owner = data.get('owner', {})\n",
    "    user = f\"{owner.get('full_name', '')} (@{owner.get('username', '')})\"\n",
    "    location = data.get('location', {})\n",
    "    location_name = location.get('name', '') if location else 'N/A'\n",
    "    captions = data.get('edge_media_to_caption', {}).get('edges', [])\n",
    "    likes = data.get('edge_media_preview_like', {}).get('count', 0)\n",
    "    comments_data = data.get('edge_media_to_parent_comment', {})\n",
    "    comments_count = comments_data.get('count', 0)\n",
    "    comments = comments_data.get('edges', [])\n",
    "    comments_list = ' | '.join([comment['node']['text'] for comment in comments])\n",
    "    tagged_users = data.get('edge_media_to_tagged_user', {}).get('edges', [])\n",
    "    tagged_brands = ', '.join([user['node']['user']['full_name'] for user in tagged_users])\n",
    "    \n",
    "    info_list = []\n",
    "    for caption in captions:\n",
    "        caption_text = caption['node']['text']\n",
    "        tags = ' '.join([tag for tag in caption_text.split() if tag.startswith('#')])\n",
    "        \n",
    "        info = {\n",
    "            'User': user,\n",
    "            'Location': location_name,\n",
    "            'Caption Text': caption_text,\n",
    "            'Tags': tags,\n",
    "            'Likes': likes,\n",
    "            'Comments Count': comments_count,\n",
    "            'Comments List': comments_list,\n",
    "            'Tagged Brands': tagged_brands\n",
    "        }\n",
    "        info_list.append(info)\n",
    "    \n",
    "    return info_list\n",
    "\n",
    "def process_raw_data():\n",
    "    data_list = []\n",
    "    for file_name in os.listdir(RAW_DATA_PATH):\n",
    "        if file_name.endswith('.info'):\n",
    "            file_path = os.path.join(RAW_DATA_PATH, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            info_list = extract_info(json_data)\n",
    "            data_list.extend(info_list)\n",
    "\n",
    "    with open(INTERIM_DATA_FILE, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['User', 'Location', 'Caption Text', 'Tags', 'Likes', 'Comments Count', 'Comments List', 'Tagged Brands']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for data in data_list:\n",
    "            writer.writerow(data)\n",
    "\n",
    "def clean_data():\n",
    "    data = pd.read_csv(INTERIM_DATA_FILE)\n",
    "    data['Cleaned Caption'] = data['Caption Text'].apply(clean_text)\n",
    "    data['Cleaned Comments'] = data['Comments List'].apply(clean_text)\n",
    "    data.dropna(subset=['Cleaned Caption'], inplace=True)\n",
    "    data.fillna('', inplace=True)\n",
    "    data.to_csv(CLEANED_DATA_FILE, index=False)\n",
    "\n",
    "def merge_with_followers():\n",
    "    try:\n",
    "        followers_df = pd.read_csv(FOLLOWERS_FILE)\n",
    "        cleaned_data_df = pd.read_csv(CLEANED_DATA_FILE)\n",
    "        \n",
    "        followers_df['Username'] = followers_df['Username'].str.strip().str.lower()\n",
    "        cleaned_data_df['User'] = cleaned_data_df['User'].str.extract(r'\\(@([^)]+)\\)')[0].str.strip().str.lower()\n",
    "        \n",
    "        merged_df = pd.merge(cleaned_data_df, followers_df[['Username', 'Followers', 'Category']], \n",
    "                             left_on='User', right_on='Username', how='left')\n",
    "        \n",
    "        if merged_df.empty:\n",
    "            return False\n",
    "        \n",
    "        merged_df.to_csv(MERGED_DATA_FILE, index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def filter_categories():\n",
    "    try:\n",
    "        merged_df = pd.read_csv(MERGED_DATA_FILE)\n",
    "        \n",
    "        categories_to_remain = ['beauty', 'family', 'fashion', 'fitness', 'food', 'travel']\n",
    "        filtered_df = merged_df[merged_df['Category'].isin(categories_to_remain)]\n",
    "        \n",
    "        if filtered_df.empty:\n",
    "            return False\n",
    "        \n",
    "        filtered_df.to_csv(FILTERED_DATA_FILE, index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def perform_sentiment_analysis():\n",
    "    df = pd.read_csv(FILTERED_DATA_FILE)\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze_sentiment(text):\n",
    "        if pd.isna(text):\n",
    "            return {'score': 0, 'category': 'Neutral'}\n",
    "        sentiment_score = sia.polarity_scores(text)['compound']\n",
    "        category = 'Positive' if sentiment_score > 0.05 else 'Negative' if sentiment_score < -0.05 else 'Neutral'\n",
    "        return {'score': sentiment_score, 'category': category}\n",
    "    \n",
    "    sentiment_results = df['Cleaned Comments'].apply(analyze_sentiment)\n",
    "    df['Sentiment Score'] = sentiment_results.apply(lambda x: x['score'])\n",
    "    df['Sentiment Category'] = sentiment_results.apply(lambda x: x['category'])\n",
    "    \n",
    "    df_positive = df[df['Sentiment Category'] == 'Positive']\n",
    "    return df_positive\n",
    "\n",
    "def categorize_and_extract_keywords(df):\n",
    "    category_keywords = {\n",
    "        'beauty': ['makeup', 'skincare', 'beauty', 'cosmetics', 'hair', 'nails', 'facial', 'lipstick', 'eyeliner', 'mascara'],\n",
    "        'fashion': ['style', 'outfit', 'fashion', 'clothes', 'accessories', 'dress', 'shoes', 'handbag', 'jewelry', 'trend'],\n",
    "        'family': ['family', 'kids', 'parenting', 'children', 'home', 'baby', 'mom', 'dad', 'sibling', 'grandparent'],\n",
    "        'fitness': ['workout', 'gym', 'fitness', 'exercise', 'health', 'muscle', 'training', 'cardio', 'strength', 'yoga'],\n",
    "        'food': ['recipe', 'cooking', 'food', 'meal', 'restaurant', 'cuisine', 'diet', 'nutrition', 'chef', 'baking'],\n",
    "        'travel': ['travel', 'vacation', 'trip', 'adventure', 'destination', 'tourism', 'hotel', 'flight', 'explore', 'sightseeing']\n",
    "    }\n",
    "    all_keywords = [word for words in category_keywords.values() for word in words]\n",
    "\n",
    "    def categorize_post(text):\n",
    "        text = str(text).lower()\n",
    "        return [category for category, keywords in category_keywords.items() if any(keyword in text for keyword in keywords)]\n",
    "\n",
    "    def get_top_keywords(text, keywords, top_n=5):\n",
    "        text = str(text).lower()\n",
    "        found_keywords = [word for word in keywords if word in text]\n",
    "        return found_keywords[:top_n]\n",
    "\n",
    "    df['categories'] = df['Cleaned Caption'].apply(categorize_post)\n",
    "    df = df[df['categories'].apply(len) > 0]\n",
    "    df['top_keywords'] = df['Cleaned Caption'].apply(lambda x: get_top_keywords(x, all_keywords))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if not os.path.exists(INTERIM_DATA_FILE):\n",
    "            process_raw_data()\n",
    "\n",
    "        clean_data()\n",
    "\n",
    "        if not merge_with_followers():\n",
    "            print(\"Merging with followers data failed. Stopping execution.\")\n",
    "            return\n",
    "\n",
    "        if not filter_categories():\n",
    "            print(\"Filtering categories failed. Stopping execution.\")\n",
    "            return\n",
    "\n",
    "        df_positive = perform_sentiment_analysis()\n",
    "        final_df = categorize_and_extract_keywords(df_positive)\n",
    "        final_df.to_csv(FINAL_DATA_FILE, index=False)\n",
    "        print(f\"Analysis complete. Final results saved to '{FINAL_DATA_FILE}'\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {str(e)}\")\n",
    "        print(\"Please make sure all required input files exist and the paths are correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nurgul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame sample:\n",
      "                                   Username     Location  \\\n",
      "0                       tomo (@lelien_tomo)          NaN   \n",
      "1           aneta sebestova (@veverkakokos)          NaN   \n",
      "2                 alice kings (@alicekings)          NaN   \n",
      "3               â˜† nicole â˜† (@nicolenic1973)       Ostsee   \n",
      "4  aleyah solomon | photographer (@aleyahs)  White Point   \n",
      "\n",
      "                                        Caption Text  \\\n",
      "0  å¤§å¥½ãã ã£ãŸã°ã‚ã¡ã‚ƒã‚“ã®ãƒŸã‚·ãƒ³ã€‚\\né«˜æ ¡ç”Ÿã®æ™‚ã¯ã‚ˆãå­¦æ ¡å¸°ã‚Šã«ã°ã‚ã¡ã‚ƒã‚“ã®å®¶ã«å¯„ã£ã¦ã€å¤•é£¯é£Ÿ...   \n",
      "1  Dneska tohle pocasi zase bylo ðŸ˜ co vic chtit.\\...   \n",
      "2                                              Quack   \n",
      "3  ðŸŒ… Im trÃ¼ben November brauche ich \"Stimmungsauf...   \n",
      "4  Island attire inspired by Alex Colville.\\n.\\n....   \n",
      "\n",
      "                                                Tags  Likes  Comments Count  \\\n",
      "0  #danielwellington #ãƒ€ãƒ‹ã‚¨ãƒ«ã‚¦ã‚§ãƒªãƒ³ãƒˆãƒ³ #myclassicdw #ã‚µãƒž...  321.0            16.0   \n",
      "1  #sunnyday #photography #nature #pond #energy #...   33.0             0.0   \n",
      "2                                                NaN   69.0             2.0   \n",
      "3  #Meer #meerliebe #vitaminsea #ostsee #ocean #l...  632.0            12.0   \n",
      "4  #capebretonisland #whitepoint #newhaven #visit...   78.0            10.0   \n",
      "\n",
      "                                       Comments List  \\\n",
      "0  æ–°å“ã¿ãŸã„ã«ç¶ºéº—ãªãƒŸã‚·ãƒ³â¤ï¸ä½•å¹´çµŒã£ã¦ã‚‚ç´ æ•µãªã‚‚ã®ã¯ç´ æ•µã ã­ðŸ˜Š | ã»ã‚“ã¨ã°ã‚ã¡ã‚ƒã‚“ã®ã”é£¯ã¦...   \n",
      "1                                                NaN   \n",
      "2                                             !! Fin   \n",
      "3  Viele wundervolle Farben und ganz viel Sonne i...   \n",
      "4  Perfect! | great shot! | Love that! | Good one...   \n",
      "\n",
      "                                       Tagged Brands  \\\n",
      "0                                  Daniel Wellington   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4  Rachel Solomon, Nova Scotia, Nikon France ðŸ‡«ðŸ‡·, ...   \n",
      "\n",
      "                                     Cleaned Caption  \\\n",
      "0  å¤§å¥½ãã ã£ãŸã°ã‚ã¡ã‚ƒã‚“ã®ãƒŸã‚·ãƒ³ é«˜æ ¡ç”Ÿã®æ™‚ã¯ã‚ˆãå­¦æ ¡å¸°ã‚Šã«ã°ã‚ã¡ã‚ƒã‚“ã®å®¶ã«å¯„ã£ã¦å¤•é£¯é£Ÿã¹ã¦å¸°...   \n",
      "1  dneska tohle pocasi zase bylo co vic chtit sun...   \n",
      "2                                              quack   \n",
      "3  im trÃ¼ben november brauche ich stimmungsaufhel...   \n",
      "4  island attire inspired alex colville photo rac...   \n",
      "\n",
      "                                    Cleaned Comments  Followers Category  \n",
      "0  æ–°å“ã¿ãŸã„ã«ç¶ºéº—ãªãƒŸã‚·ãƒ³ä½•å¹´çµŒã£ã¦ã‚‚ç´ æ•µãªã‚‚ã®ã¯ç´ æ•µã ã­ ã»ã‚“ã¨ã°ã‚ã¡ã‚ƒã‚“ã®ã”é£¯ã¦ãªã‚“ã§ã‚ã‚“...        NaN      NaN  \n",
      "1                                                NaN        NaN      NaN  \n",
      "2                                                fin        NaN      NaN  \n",
      "3  viele wundervolle farben und ganz viel sonne i...        NaN      NaN  \n",
      "4  perfect great shot love good one loooove paint...        NaN      NaN  \n",
      "Followers and Category added to the cleaned Instagram data successfully.\n",
      "Influencers per category sample:\n",
      "Empty DataFrame\n",
      "Columns: [Category, Number of Influencers]\n",
      "Index: []\n",
      "Number of influencers in each category calculated and saved successfully.\n",
      "Categories before filtering: [nan]\n",
      "Filtered DataFrame sample:\n",
      "Empty DataFrame\n",
      "Columns: [Username, Location, Caption Text, Tags, Likes, Comments Count, Comments List, Tagged Brands, Cleaned Caption, Cleaned Comments, Followers, Category]\n",
      "Index: []\n",
      "Filtered data saved successfully.\n",
      "DataFrame sample before sentiment analysis:\n",
      "Empty DataFrame\n",
      "Columns: [Username, Location, Caption Text, Tags, Likes, Comments Count, Comments List, Tagged Brands, Cleaned Caption, Cleaned Comments, Followers, Category]\n",
      "Index: []\n",
      "Positive sentiment DataFrame sample:\n",
      "Empty DataFrame\n",
      "Columns: [Username, Location, Caption Text, Tags, Likes, Comments Count, Comments List, Tagged Brands, Cleaned Caption, Cleaned Comments, Followers, Category, Sentiment Score, Sentiment Category]\n",
      "Index: []\n",
      "Sentiment analysis complete. Positive comments saved.\n",
      "DataFrame sample before categorization:\n",
      "Empty DataFrame\n",
      "Columns: [Username, Location, Caption Text, Tags, Likes, Comments Count, Comments List, Tagged Brands, Cleaned Caption, Cleaned Comments, Followers, Category, Sentiment Score, Sentiment Category]\n",
      "Index: []\n",
      "Categorized DataFrame sample:\n",
      "Empty DataFrame\n",
      "Columns: [Username, Location, Caption Text, Tags, Likes, Comments Count, Comments List, Tagged Brands, Cleaned Caption, Cleaned Comments, Followers, Category, Sentiment Score, Sentiment Category, categories, top_keywords]\n",
      "Index: []\n",
      "Keyword analysis complete. Results saved to '/Users/nurgul/Documents/Projects/Dissertation Code/data/4.Final_instagram_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from langdetect import detect, LangDetectException\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# File paths\n",
    "BASE_PATH = os.path.expanduser(\"~/Documents/Projects/Dissertation Code/data\")\n",
    "RAW_DATA_PATH = os.path.expanduser(\"~/Library/CloudStorage/OneDrive-UniversityofEastLondon/DS7010_Dissertation/Data/info\")\n",
    "FOLLOWERS_FILE = os.path.join(BASE_PATH, \"Number of followers for each influencer.csv\")\n",
    "INTERIM_DATA_FILE = os.path.join(BASE_PATH, \"instagram_data.csv\")\n",
    "CLEANED_DATA_FILE = os.path.join(BASE_PATH, \"1.Cleaned_instagram_data.csv\")\n",
    "MERGED_DATA_FILE = os.path.join(BASE_PATH, \"2.Merged_instagram_data.csv\")\n",
    "FILTERED_DATA_FILE = os.path.join(BASE_PATH, \"3.Filtered_instagram_data.csv\")\n",
    "FINAL_DATA_FILE = os.path.join(BASE_PATH, \"4.Final_instagram_data.csv\")\n",
    "\n",
    "def extract_info(data):\n",
    "    info_list = []\n",
    "    owner = data.get('owner', {})\n",
    "    user = f\"{owner.get('full_name', '')} (@{owner.get('username', '')})\"\n",
    "    location = data.get('location') or {}\n",
    "    location_name = location.get('name', 'N/A')\n",
    "    captions = data.get('edge_media_to_caption', {}).get('edges', [])\n",
    "    likes = data.get('edge_media_preview_like', {}).get('count', 0)\n",
    "    comments_data = data.get('edge_media_to_parent_comment', {})\n",
    "    comments_count = comments_data.get('count', 0)\n",
    "    comments_list = ' | '.join(comment['node']['text'] for comment in comments_data.get('edges', []))\n",
    "    tagged_brands = ', '.join(user['node']['user']['full_name'] for user in data.get('edge_media_to_tagged_user', {}).get('edges', []))\n",
    "\n",
    "    for caption in captions:\n",
    "        caption_text = caption['node']['text']\n",
    "        tags = ' '.join(tag for tag in caption_text.split() if tag.startswith('#'))\n",
    "        info_list.append({\n",
    "            'Username': owner.get('username', '').lower().strip(),\n",
    "            'Location': location_name,\n",
    "            'Caption Text': caption_text,\n",
    "            'Tags': tags,\n",
    "            'Likes': likes,\n",
    "            'Comments Count': comments_count,\n",
    "            'Comments List': comments_list,\n",
    "            'Tagged Brands': tagged_brands\n",
    "        })\n",
    "\n",
    "    return info_list\n",
    "\n",
    "def process_json_files(input_dir, output_csv):\n",
    "    data_list = []\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.info'):\n",
    "            with open(os.path.join(input_dir, file_name), 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            data_list.extend(extract_info(json_data))\n",
    "\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['Username', 'Location', 'Caption Text', 'Tags', 'Likes', 'Comments Count', 'Comments List', 'Tagged Brands']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for data in data_list:\n",
    "            writer.writerow(data)\n",
    "\n",
    "    print(\"Data extraction and CSV file creation completed successfully.\")\n",
    "    print(f\"Extracted data sample: {data_list[:5]}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, float):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text).lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "def clean_instagram_data(input_csv, output_csv):\n",
    "    data = pd.read_csv(input_csv)\n",
    "    data['Cleaned Caption'] = data['Caption Text'].apply(clean_text)\n",
    "    data['Cleaned Comments'] = data['Comments List'].apply(clean_text)\n",
    "    data.dropna(subset=['Cleaned Caption'], inplace=True)\n",
    "    data.fillna('', inplace=True)\n",
    "    data.to_csv(output_csv, index=False)\n",
    "    print(\"Data cleaning completed successfully.\")\n",
    "    print(f\"Cleaned data sample: {data.head()}\")\n",
    "\n",
    "def merge_followers_data(cleaned_csv, followers_csv, output_csv):\n",
    "    cleaned_data_df = pd.read_csv(cleaned_csv)\n",
    "    followers_df = pd.read_csv(followers_csv)\n",
    "    \n",
    "    # Clean column names\n",
    "    cleaned_data_df.columns = cleaned_data_df.columns.str.strip()\n",
    "    followers_df.columns = followers_df.columns.str.strip()\n",
    "    \n",
    "    # Ensure 'Username' column exists in both dataframes\n",
    "    if 'Username' not in cleaned_data_df.columns or 'Username' not in followers_df.columns:\n",
    "        print(\"Error: 'Username' column is missing in one of the data files.\")\n",
    "        return\n",
    "    \n",
    "    # Normalize 'Username' columns\n",
    "    cleaned_data_df['Username'] = cleaned_data_df['Username'].str.strip().str.lower()\n",
    "    followers_df['Username'] = followers_df['Username'].str.strip().str.lower()\n",
    "    \n",
    "    # Merge DataFrames\n",
    "    merged_df = pd.merge(cleaned_data_df, followers_df[['Username', 'Followers', 'Category']], on='Username', how='left')\n",
    "    \n",
    "    # Print sample of merged data\n",
    "    print(\"Merged DataFrame sample:\")\n",
    "    print(merged_df.head())\n",
    "    \n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "    print(\"Followers and Category added to the cleaned Instagram data successfully.\")\n",
    "\n",
    "def filter_non_english_rows(input_csv, output_csv, text_column):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    if text_column in df.columns:\n",
    "        initial_count = len(df)\n",
    "        df = df[df[text_column].apply(lambda x: len(str(x).strip()) > 0 and is_english(x))]\n",
    "        final_count = len(df)\n",
    "        print(f\"Removed {initial_count - final_count} non-English rows.\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(\"Non-English rows were removed successfully.\")\n",
    "    print(f\"Filtered data sample: {df.head()}\")\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def calculate_influencers_per_category(merged_csv, output_csv):\n",
    "    merged_df = pd.read_csv(merged_csv)\n",
    "    \n",
    "    influencers_per_category = merged_df.groupby('Category')['Username'].nunique().reset_index()\n",
    "    influencers_per_category.columns = ['Category', 'Number of Influencers']\n",
    "    \n",
    "    print(\"Influencers per category sample:\")\n",
    "    print(influencers_per_category.head())\n",
    "    \n",
    "    influencers_per_category.to_csv(output_csv, index=False)\n",
    "    print(\"Number of influencers in each category calculated and saved successfully.\")\n",
    "\n",
    "def filter_categories(input_csv, output_csv, categories):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    print(\"Categories before filtering:\", df['Category'].unique())\n",
    "    filtered_df = df[df['Category'].isin(categories)]\n",
    "    \n",
    "    print(\"Filtered DataFrame sample:\")\n",
    "    print(filtered_df.head())\n",
    "    \n",
    "    filtered_df.to_csv(output_csv, index=False)\n",
    "    print(\"Filtered data saved successfully.\")\n",
    "\n",
    "def analyze_sentiment(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    print(\"DataFrame sample before sentiment analysis:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    df['Sentiment Score'] = df['Cleaned Comments'].apply(lambda x: sia.polarity_scores(x)['compound'] if isinstance(x, str) else 0)\n",
    "    df['Sentiment Category'] = df['Sentiment Score'].apply(lambda x: 'Positive' if x > 0.05 else 'Negative' if x < -0.05 else 'Neutral')\n",
    "    df_positive = df[df['Sentiment Category'] == 'Positive']\n",
    "    \n",
    "    print(\"Positive sentiment DataFrame sample:\")\n",
    "    print(df_positive.head())\n",
    "    \n",
    "    df_positive.to_csv(output_csv, index=False)\n",
    "    print(\"Sentiment analysis complete. Positive comments saved.\")\n",
    "\n",
    "def categorize_instagram_data(input_csv, output_csv, method='predefined'):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    print(\"DataFrame sample before categorization:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    category_keywords = {\n",
    "        'beauty': ['makeup', 'skincare', 'beauty', 'cosmetics', 'hair', 'nails', 'facial', 'lipstick', 'eyeliner', 'mascara'],\n",
    "        'fashion': ['style', 'outfit', 'fashion', 'clothes', 'accessories', 'dress', 'shoes', 'handbag', 'jewelry', 'trend'],\n",
    "        'family': ['family', 'kids', 'parenting', 'children', 'home', 'baby', 'mom', 'dad', 'sibling', 'grandparent'],\n",
    "        'fitness': ['workout', 'gym', 'fitness', 'exercise', 'health', 'muscle', 'training', 'cardio', 'strength', 'yoga'],\n",
    "        'food': ['recipe', 'cooking', 'food', 'meal', 'restaurant', 'cuisine', 'diet', 'nutrition', 'chef', 'baking'],\n",
    "        'travel': ['travel', 'vacation', 'trip', 'adventure', 'destination', 'tourism', 'hotel', 'flight', 'explore', 'sightseeing']\n",
    "    }\n",
    "    all_keywords = [word for words in category_keywords.values() for word in words]\n",
    "\n",
    "    def categorize_post(text):\n",
    "        text = str(text).lower()\n",
    "        categories = [category for category, keywords in category_keywords.items() if any(keyword in text for keyword in keywords)]\n",
    "        return categories\n",
    "\n",
    "    df['categories'] = df['Cleaned Caption'].apply(categorize_post)\n",
    "    df = df[df['categories'].apply(len) > 0]\n",
    "\n",
    "    def get_top_keywords_predefined(text, keywords, top_n=5):\n",
    "        text = str(text).lower()\n",
    "        found_keywords = [word for word in keywords if word in text]\n",
    "        return found_keywords[:top_n]\n",
    "\n",
    "    if method == 'predefined':\n",
    "        df['top_keywords'] = df['Cleaned Caption'].apply(lambda x: get_top_keywords_predefined(x, all_keywords))\n",
    "    else:\n",
    "        tfidf = TfidfVectorizer(max_features=1000)\n",
    "        tfidf_matrix = tfidf.fit_transform(df['Cleaned Caption'].fillna(''))\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        df['top_keywords'] = [get_top_keywords_tfidf(tfidf_matrix[i], feature_names) for i in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "    print(\"Categorized DataFrame sample:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Keyword analysis complete. Results saved to '{output_csv}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # process_json_files(RAW_DATA_PATH, INTERIM_DATA_FILE)\n",
    "    # clean_instagram_data(INTERIM_DATA_FILE, CLEANED_DATA_FILE)\n",
    "    # filter_non_english_rows(CLEANED_DATA_FILE, CLEANED_DATA_FILE, 'Caption Text')\n",
    "    merge_followers_data(CLEANED_DATA_FILE, FOLLOWERS_FILE, MERGED_DATA_FILE)\n",
    "    calculate_influencers_per_category(MERGED_DATA_FILE, os.path.join(BASE_PATH, \"influencers_per_category.csv\"))\n",
    "    filter_categories(MERGED_DATA_FILE, FILTERED_DATA_FILE, ['beauty', 'family', 'fashion', 'fitness', 'food', 'travel'])\n",
    "    analyze_sentiment(FILTERED_DATA_FILE, os.path.join(BASE_PATH, \"instagram_comments_positive_only.csv\"))\n",
    "    categorize_instagram_data(os.path.join(BASE_PATH, \"instagram_comments_positive_only.csv\"), FINAL_DATA_FILE, method='predefined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
