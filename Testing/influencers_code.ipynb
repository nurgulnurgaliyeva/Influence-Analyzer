{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Objective**: Extract data from JSON files containing Instagram post metadata.\n",
    "- **Process**:\n",
    "    - Load JSON files into a Python script.\n",
    "    - Parse the JSON structure to extract relevant fields such as captions, user tags, hashtags, timestamps, sponsorship status, number of likes, and comments.\n",
    "    - Store the extracted data in a structured format, such as a Pandas DataFrame, for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and CSV file creation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Function to extract details from the JSON data\n",
    "def extract_info(data):\n",
    "    info_list = []\n",
    "    \n",
    "    # Extracting user information\n",
    "    owner = data.get('owner', {})\n",
    "    user = f\"{owner.get('full_name', '')} (@{owner.get('username', '')})\"\n",
    "    \n",
    "    # Extracting location information\n",
    "    location = data.get('location', {})\n",
    "    location_name = location.get('name', '') if location else 'N/A'\n",
    "    \n",
    "    # Extracting caption highlights and tags\n",
    "    captions = data.get('edge_media_to_caption', {}).get('edges', [])\n",
    "    \n",
    "    # Extracting engagement details\n",
    "    likes = data.get('edge_media_preview_like', {}).get('count', 0)\n",
    "    comments_data = data.get('edge_media_to_parent_comment', {})\n",
    "    comments_count = comments_data.get('count', 0)\n",
    "    comments = comments_data.get('edges', [])\n",
    "    comments_list = ' | '.join([comment['node']['text'] for comment in comments])\n",
    "    \n",
    "    # Extracting tagged brands\n",
    "    tagged_users = data.get('edge_media_to_tagged_user', {}).get('edges', [])\n",
    "    tagged_brands = ', '.join([user['node']['user']['full_name'] for user in tagged_users])\n",
    "    \n",
    "    for caption in captions:\n",
    "        caption_text = caption['node']['text']\n",
    "        tags = ' '.join([tag for tag in caption_text.split() if tag.startswith('#')])\n",
    "        \n",
    "        info = {\n",
    "            'User': user,\n",
    "            'Location': location_name,\n",
    "            'Caption Text': caption_text,\n",
    "            'Tags': tags,\n",
    "            'Likes': likes,\n",
    "            'Comments Count': comments_count,\n",
    "            'Comments List': comments_list,\n",
    "            'Tagged Brands': tagged_brands\n",
    "        }\n",
    "        \n",
    "        info_list.append(info)\n",
    "    \n",
    "    return info_list\n",
    "\n",
    "# Define the path where the JSON files are stored\n",
    "input_path = \"/Users/nurgul/Library/CloudStorage/OneDrive-UniversityofEastLondon/DS7010_Dissertation/Data/info\"\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/instagram_data.csv\"\n",
    "\n",
    "# List to store all the extracted information\n",
    "data_list = []\n",
    "\n",
    "# Process each file in the input directory\n",
    "for file_name in os.listdir(input_path):\n",
    "    if file_name.endswith('.info'):\n",
    "        file_path = os.path.join(input_path, file_name)\n",
    "        \n",
    "        # Read the JSON data from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Extract information from the JSON data\n",
    "        info_list = extract_info(json_data)\n",
    "        \n",
    "        # Append the extracted information to the data list\n",
    "        data_list.extend(info_list)\n",
    "\n",
    "# Write the extracted information to the CSV file\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = ['User', 'Location', 'Caption Text', 'Tags', 'Likes', 'Comments Count', 'Comments List', 'Tagged Brands']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write the data rows\n",
    "    for data in data_list:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(\"Data extraction and CSV file creation completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Objective**: Clean and preprocess the extracted data to ensure consistency and quality.\n",
    "- **Process**:\n",
    "    - **Text Normalization**: Remove special characters, emojis, and stopwords from captions and comments using libraries like NLTK and SpaCy.\n",
    "    - **Metadata Cleaning**: Handle missing data by imputing or removing incomplete records.\n",
    "    - **Column Renaming**: Standardize column names by stripping leading and trailing spaces and converting them to a consistent case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nurgul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have downloaded the stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, float):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Load the CSV data\n",
    "data = pd.read_csv('/Users/nurgul/Documents/Projects/Dissertation Code/data/instagram_data.csv')\n",
    "\n",
    "# Clean the 'Caption Text' and 'Comments List' columns\n",
    "data['Cleaned Caption'] = data['Caption Text'].apply(clean_text)\n",
    "data['Cleaned Comments'] = data['Comments List'].apply(clean_text)\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(subset=['Cleaned Caption'], inplace=True)\n",
    "data.fillna('', inplace=True)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_output_csv = '/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data.csv'\n",
    "data.to_csv(cleaned_output_csv, index=False)\n",
    "\n",
    "print(\"Data cleaning completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Objective**: Integrate the number of followers for each influencer into the cleaned Instagram data.\n",
    "- **Process**:\n",
    "    - Load the followers data from a CSV file.\n",
    "    - Clean the followers data by stripping spaces and converting usernames to lowercase ***(file name: Cleaning the data step 2; cleaned_instagram_data_cleaned.csv)*.**\n",
    "    - Merge the cleaned followers data with the Instagram data based on the username, ensuring the correct alignment of data.\n",
    "    - Save the merged data to a new CSV file ***(file name: Merging cleaned datas.py; cleaned_instagram_data_final.csv)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Columns in cleaned_data_df: Index(['User', 'Location', 'Caption Text', 'Tags', 'Likes', 'Comments Count',\n",
      "       'Comments List', 'Tagged Brands', 'Cleaned Caption',\n",
      "       'Cleaned Comments'],\n",
      "      dtype='object')\n",
      "'User' column cleaned, renamed to 'Cleaned User', and original 'User' column removed\n",
      "'captions' column not found in cleaned_data_df\n",
      "Cleaned Instagram data saved successfully.\n",
      "Final columns in cleaned_data_df: Index(['Cleaned User', 'Location', 'Likes', 'Tags', 'Comments Count',\n",
      "       'Comments List', 'Tagged Brands', 'Cleaned Caption'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the file path for cleaned_instagram_data\n",
    "cleaned_data_file = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "cleaned_data_df = pd.read_csv(cleaned_data_file)\n",
    "\n",
    "# Print the column names to debug\n",
    "print(\"Original Columns in cleaned_data_df:\", cleaned_data_df.columns)\n",
    "\n",
    "# Ensure column names are stripped of extra spaces\n",
    "cleaned_data_df.columns = cleaned_data_df.columns.str.strip()\n",
    "\n",
    "# Rename columns to remove leading/trailing spaces\n",
    "cleaned_data_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        # Remove emoji and other special characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Clean and rename 'User' column\n",
    "if 'User' in cleaned_data_df.columns:\n",
    "    cleaned_data_df['Cleaned User'] = cleaned_data_df['User'].str.strip().str.lower()\n",
    "    cleaned_data_df['Cleaned User'] = cleaned_data_df['Cleaned User'].str.extract(r'\\((@[\\w\\._]+)\\)')[0]\n",
    "    cleaned_data_df['Cleaned User'] = cleaned_data_df['Cleaned User'].str.replace('@', '').str.strip()\n",
    "    cleaned_data_df = cleaned_data_df.drop('User', axis=1)\n",
    "    print(\"'User' column cleaned, renamed to 'Cleaned User', and original 'User' column removed\")\n",
    "else:\n",
    "    print(\"'User' column not found in cleaned_data_df\")\n",
    "\n",
    "# Clean 'captions' and add a new 'Cleaned Captions' column\n",
    "if 'captions' in cleaned_data_df.columns:\n",
    "    cleaned_data_df['Cleaned Captions'] = cleaned_data_df['captions'].apply(clean_text)\n",
    "    cleaned_data_df = cleaned_data_df.drop('captions', axis=1)\n",
    "    print(\"Original 'captions' column deleted and replaced with 'Cleaned Captions'\")\n",
    "else:\n",
    "    print(\"'captions' column not found in cleaned_data_df\")\n",
    "\n",
    "# Drop Cleaned Comments and Caption Text columns\n",
    "cleaned_data_df = cleaned_data_df.drop(['Cleaned Comments', 'Caption Text'], axis=1)\n",
    "\n",
    "# Define the desired column order\n",
    "desired_order = ['Cleaned User', 'Location', 'Cleaned Captions', 'Likes', 'Comments count', 'Comments list', 'Tagged brands', 'Tags']\n",
    "\n",
    "# Create a new list with only the columns that exist in the dataframe, in the specified order\n",
    "new_order = [col for col in desired_order if col in cleaned_data_df.columns]\n",
    "\n",
    "# Add any remaining columns that weren't in the desired_order list\n",
    "remaining_columns = [col for col in cleaned_data_df.columns if col not in new_order]\n",
    "final_order = new_order + remaining_columns\n",
    "\n",
    "# Reorder the columns\n",
    "cleaned_data_df = cleaned_data_df[final_order]\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file\n",
    "cleaned_data_df.to_csv(\"/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data_cleaned.csv\", index=False)\n",
    "print(\"Cleaned Instagram data saved successfully.\")\n",
    "\n",
    "# Print the final column names\n",
    "print(\"Final columns in cleaned_data_df:\", cleaned_data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "followers_file = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/Number of followers for each influencer.csv\"\n",
    "cleaned_data_file = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data_cleaned.csv\"\n",
    "\n",
    "# Read the CSV files\n",
    "followers_df = pd.read_csv(followers_file)\n",
    "cleaned_data_df = pd.read_csv(cleaned_data_file)\n",
    "\n",
    "# Ensure column names are stripped of extra spaces\n",
    "followers_df.columns = followers_df.columns.str.strip()\n",
    "cleaned_data_df.columns = cleaned_data_df.columns.str.strip()\n",
    "\n",
    "# Strip leading/trailing spaces in usernames\n",
    "followers_df['Username'] = followers_df['Username'].str.strip()\n",
    "cleaned_data_df['User'] = cleaned_data_df['User'].str.strip()\n",
    "\n",
    "# Convert usernames to lower case to avoid case sensitivity issues\n",
    "followers_df['Username'] = followers_df['Username'].str.lower()\n",
    "cleaned_data_df['User'] = cleaned_data_df['User'].str.lower()\n",
    "\n",
    "# Merge the two dataframes on the username column, including the Category column\n",
    "merged_df = pd.merge(cleaned_data_df, followers_df[['Username', 'Followers', 'Category']], left_on='User', right_on='Username', how='left')\n",
    "\n",
    "# Check if the merge was successful by printing some rows with followers and category\n",
    "print(\"Sample rows from merged_df with followers and category:\")\n",
    "print(merged_df[~merged_df['Followers'].isna()].head())\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv(\"/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data_final.csv\", index=False)\n",
    "\n",
    "print(\"Followers and Category added to the cleaned Instagram data successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4. Removing non-english captions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in cleaned_data_df: Index(['User', 'Location', 'Caption Text', 'Tags', 'Likes', 'Comments Count',\n",
      "       'Comments List', 'Tagged Brands', 'Cleaned Caption',\n",
      "       'Cleaned Comments'],\n",
      "      dtype='object')\n",
      "Removed 9230 non-English rows.\n",
      "Sample rows from merged_df with followers and category:\n",
      "               User     Location  \\\n",
      "0           aleyahs  White Point   \n",
      "1         pxl.house          NaN   \n",
      "2        merakilane          NaN   \n",
      "3      zanabfarooq_          NaN   \n",
      "4  lightscamerabake          NaN   \n",
      "\n",
      "                                        Caption Text  \\\n",
      "0  Island attire inspired by Alex Colville.\\n.\\n....   \n",
      "1  Photograph for @reebokwomen's fall issue of RA...   \n",
      "2  One of my Friday afternoon rituals is to sit d...   \n",
      "3                     Tis the season to be jolly ðŸŽ„ðŸŽ.   \n",
      "4  {NEW} on the #blog today! Peanut Butter Bars ðŸ’•...   \n",
      "\n",
      "                                                Tags  Likes  Comments Count  \\\n",
      "0  #capebretonisland #whitepoint #newhaven #visit...   78.0            10.0   \n",
      "1                                      #ReebokRALLYâ €   58.0             2.0   \n",
      "2                                                NaN  198.0             8.0   \n",
      "3                                                NaN  568.0            17.0   \n",
      "4                                      #blog #recipe   78.0             7.0   \n",
      "\n",
      "                                       Comments List  \\\n",
      "0  Perfect! | great shot! | Love that! | Good one...   \n",
      "1                                    Niceeeee! | ðŸ’¥ðŸ’¥ðŸ“¸   \n",
      "2  .\\n.\\n.\\n.\\n#MerakiLane #lifeisgood #behappy #...   \n",
      "3  Omg i love the whole Christmasy vibes | Itâ€™s s...   \n",
      "4  #lightscamerabake #foodporn #foodofinsta #inst...   \n",
      "\n",
      "                                       Tagged Brands  \\\n",
      "0  Rachel Solomon, Nova Scotia, Nikon France ðŸ‡«ðŸ‡·, ...   \n",
      "1  D A V I D  S A L A F I A, Reebok, Janet Hender...   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                     Cleaned Caption  \\\n",
      "0  island attire inspired alex colville photo rac...   \n",
      "1  photograph reebokwomens fall issue rally magaz...   \n",
      "2  one friday afternoon rituals sit write list go...   \n",
      "3                                   tis season jolly   \n",
      "4  new blog today peanut butter bars link bio recipe   \n",
      "\n",
      "                                    Cleaned Comments          Username  \\\n",
      "0  perfect great shot love good one loooove paint...           aleyahs   \n",
      "1                                           niceeeee         pxl.house   \n",
      "2  merakilane lifeisgood behappy thatsdarling lif...        merakilane   \n",
      "3  omg love whole christmasy vibes fun seeing dec...      zanabfarooq_   \n",
      "4  lightscamerabake foodporn foodofinsta instafoo...  lightscamerabake   \n",
      "\n",
      "   Followers Category  \n",
      "0     2063.0   travel  \n",
      "1     1109.0  fashion  \n",
      "2    15464.0     food  \n",
      "3    17543.0  fashion  \n",
      "4     1361.0     food  \n",
      "Non-english rows were removed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# Define the file paths\n",
    "followers_file = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/Number of followers for each influencer.csv\"\n",
    "cleaned_data_file = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data_cleaned.csv\"\n",
    "\n",
    "# Read the CSV files\n",
    "followers_df = pd.read_csv(followers_file)\n",
    "cleaned_data_df = pd.read_csv(cleaned_data_file)\n",
    "\n",
    "# Ensure column names are stripped of extra spaces\n",
    "followers_df.columns = followers_df.columns.str.strip()\n",
    "cleaned_data_df.columns = cleaned_data_df.columns.str.strip()\n",
    "\n",
    "# Strip leading/trailing spaces in usernames\n",
    "followers_df['Username'] = followers_df['Username'].str.strip()\n",
    "cleaned_data_df['User'] = cleaned_data_df['User'].str.strip()\n",
    "\n",
    "# Convert usernames to lower case to avoid case sensitivity issues\n",
    "followers_df['Username'] = followers_df['Username'].str.lower()\n",
    "cleaned_data_df['User'] = cleaned_data_df['User'].str.lower()\n",
    "\n",
    "# Print column names to verify the correct text column\n",
    "print(\"Column names in cleaned_data_df:\", cleaned_data_df.columns)\n",
    "\n",
    "# Function to detect language and filter out non-English rows\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# Verify the column name that contains the text\n",
    "text_column = 'Caption Text'  # Update this to the correct column name after inspecting the columns\n",
    "\n",
    "if text_column in cleaned_data_df.columns:\n",
    "    # Count the number of non-English rows\n",
    "    initial_count = len(cleaned_data_df)\n",
    "    cleaned_data_df = cleaned_data_df[cleaned_data_df[text_column].apply(is_english)]\n",
    "    final_count = len(cleaned_data_df)\n",
    "    non_english_count = initial_count - final_count\n",
    "    print(f\"Removed {non_english_count} non-English rows.\")\n",
    "else:\n",
    "    print(f\"Column '{text_column}' not found in the DataFrame.\")\n",
    "\n",
    "# Merge the two dataframes on the username column, including the Category column\n",
    "merged_df = pd.merge(cleaned_data_df, followers_df[['Username', 'Followers', 'Category']], left_on='User', right_on='Username', how='left')\n",
    "\n",
    "# Check if the merge was successful by printing some rows with followers and category\n",
    "print(\"Sample rows from merged_df with followers and category:\")\n",
    "print(merged_df[~merged_df['Followers'].isna()].head())\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv(\"/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data_final.csv\", index=False)\n",
    "\n",
    "print(\"Non-english rows were removed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5. Calculating number of influencers per each category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of influencers in each category:\n",
      "   Category  Number of Influencers\n",
      "0    beauty                    771\n",
      "1    family                   2153\n",
      "2   fashion                   6231\n",
      "3    fasion                      1\n",
      "4   fitness                    616\n",
      "5      food                   2038\n",
      "6  interior                    646\n",
      "7     other                   3050\n",
      "8       pet                    349\n",
      "9    travel                   2303\n",
      "Number of influencers in each category calculated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path for the merged data\n",
    "merged_data_file = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data_final.csv\"\n",
    "\n",
    "# Read the merged CSV file\n",
    "merged_df = pd.read_csv(merged_data_file)\n",
    "\n",
    "# Calculate the number of influencers in each category\n",
    "influencers_per_category = merged_df.groupby('Category')['Username'].nunique().reset_index()\n",
    "influencers_per_category.columns = ['Category', 'Number of Influencers']\n",
    "\n",
    "# Print the number of influencers in each category\n",
    "print(\"Number of influencers in each category:\")\n",
    "print(influencers_per_category)\n",
    "\n",
    "# Save the influencers per category to a new CSV file\n",
    "influencers_per_category.to_csv(\"/Users/nurgul/Documents/Projects/Dissertation Code/data/influencers_per_category.csv\", index=False)\n",
    "\n",
    "print(\"Number of influencers in each category calculated and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6. Remain only necessary categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path for the merged data\n",
    "merged_data_file = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data_final.csv\"\n",
    "\n",
    "# Read the merged CSV file\n",
    "merged_df = pd.read_csv(merged_data_file)\n",
    "\n",
    "# Define the categories to remain\n",
    "categories_to_remain = ['beauty', 'family', 'fashion', 'fitness', 'food', 'travel']\n",
    "\n",
    "# Filter the DataFrame to include only the specified categories\n",
    "filtered_df = merged_df[merged_df['Category'].isin(categories_to_remain)]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_data_file = \"/Users/nurgul/Documents/Projects/Dissertation Code/data/cleaned_instagram_data_filtered.csv\"\n",
    "filtered_df.to_csv(filtered_data_file, index=False)\n",
    "\n",
    "print(\"Filtered data saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7. Sentiment analysis** - positive only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Cleaned Comments  Sentiment Score  \\\n",
      "0  perfect great shot love good one loooove paint...           0.9565   \n",
      "2  merakilane lifeisgood behappy thatsdarling lif...           0.9694   \n",
      "3  omg love whole christmasy vibes fun seeing dec...           0.9970   \n",
      "4  lightscamerabake foodporn foodofinsta instafoo...           0.9769   \n",
      "5  haha caption though law handsome super model m...           0.9719   \n",
      "\n",
      "  Sentiment Category  \n",
      "0           Positive  \n",
      "2           Positive  \n",
      "3           Positive  \n",
      "4           Positive  \n",
      "5           Positive  \n",
      "\n",
      "Positive Sentiment Distribution:\n",
      "Sentiment Category\n",
      "Positive    100.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sentiment analysis complete. Positive comments saved to '/Users/nurgul/Documents/Projects/Dissertation Code/data/instagram_comments_positive_only.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "\n",
    "# Download the NLTK sentiment analysis model\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Set the file path and name\n",
    "file_path = \"/Users/nurgul/Documents/Projects/Dissertation Code/data\"\n",
    "file_name = \"cleaned_instagram_data_filtered.csv\"\n",
    "full_path = os.path.join(file_path, file_name)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(full_path)\n",
    "\n",
    "# Initialize the NLTK sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to perform sentiment analysis on a text\n",
    "def analyze_sentiment(text):\n",
    "    if pd.isna(text):\n",
    "        return {'score': 0, 'category': 'Neutral'}  # Return neutral sentiment for empty/NaN values\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    \n",
    "    if sentiment_score > 0.05:\n",
    "        category = 'Positive'\n",
    "    elif sentiment_score < -0.05:\n",
    "        category = 'Negative'\n",
    "    else:\n",
    "        category = 'Neutral'\n",
    "    \n",
    "    return {'score': sentiment_score, 'category': category}\n",
    "\n",
    "# Perform sentiment analysis on the 'Cleaned Comments' column\n",
    "sentiment_results = df['Cleaned Comments'].apply(analyze_sentiment)\n",
    "\n",
    "# Add new columns for Sentiment Score and Sentiment Category\n",
    "df['Sentiment Score'] = sentiment_results.apply(lambda x: x['score'])\n",
    "df['Sentiment Category'] = sentiment_results.apply(lambda x: x['category'])\n",
    "\n",
    "# Filter to keep only positive rows\n",
    "df_positive = df[df['Sentiment Category'] == 'Positive']\n",
    "\n",
    "# Display the first few rows of the updated dataset (positive only)\n",
    "print(df_positive[['Cleaned Comments', 'Sentiment Score', 'Sentiment Category']].head())\n",
    "\n",
    "# Calculate and display overall sentiment statistics for positive comments\n",
    "sentiment_stats = df_positive['Sentiment Category'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPositive Sentiment Distribution:\")\n",
    "print(sentiment_stats)\n",
    "\n",
    "# Save the updated dataset (positive only) to a new CSV file\n",
    "output_file = os.path.join(file_path, \"instagram_comments_positive_only.csv\")\n",
    "df_positive.to_csv(output_file, index=False)\n",
    "print(f\"\\nSentiment analysis complete. Positive comments saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8. Keywords analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Distribution:\n",
      "categories\n",
      "fashion    0.258260\n",
      "family     0.231107\n",
      "travel     0.155353\n",
      "food       0.133486\n",
      "beauty     0.130902\n",
      "fitness    0.090893\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample Top Keywords:\n",
      "0    [vacation, trip, adventure, destination, explore]\n",
      "3                                             [recipe]\n",
      "5                                            [fashion]\n",
      "7                                            [explore]\n",
      "8                          [dress, health, food, chef]\n",
      "Name: top_keywords, dtype: object\n",
      "\n",
      "Keyword analysis complete. Results saved to '/Users/nurgul/Documents/Projects/Dissertation Code/data/categorized_instagram_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "\n",
    "# Set the file path\n",
    "file_path = \"/Users/nurgul/Documents/Projects/Dissertation Code/data\"\n",
    "input_file = \"instagram_comments_positive_only.csv\"\n",
    "output_file = \"categorized_instagram_data.csv\"\n",
    "\n",
    "# Load the data\n",
    "full_input_path = os.path.join(file_path, input_file)\n",
    "df = pd.read_csv(full_input_path)\n",
    "\n",
    "# Define category-specific keywords\n",
    "category_keywords = {\n",
    "    'beauty': ['makeup', 'skincare', 'beauty', 'cosmetics', 'hair', 'nails', 'facial', 'lipstick', 'eyeliner', 'mascara'],\n",
    "    'fashion': ['style', 'outfit', 'fashion', 'clothes', 'accessories', 'dress', 'shoes', 'handbag', 'jewelry', 'trend'],\n",
    "    'family': ['family', 'kids', 'parenting', 'children', 'home', 'baby', 'mom', 'dad', 'sibling', 'grandparent'],\n",
    "    'fitness': ['workout', 'gym', 'fitness', 'exercise', 'health', 'muscle', 'training', 'cardio', 'strength', 'yoga'],\n",
    "    'food': ['recipe', 'cooking', 'food', 'meal', 'restaurant', 'cuisine', 'diet', 'nutrition', 'chef', 'baking'],\n",
    "    'travel': ['travel', 'vacation', 'trip', 'adventure', 'destination', 'tourism', 'hotel', 'flight', 'explore', 'sightseeing']\n",
    "}\n",
    "\n",
    "# Flatten the list of keywords\n",
    "all_keywords = [word for words in category_keywords.values() for word in words]\n",
    "\n",
    "# Function to categorize post based on keywords\n",
    "def categorize_post(text):\n",
    "    text = str(text).lower()  # Convert to string and lowercase\n",
    "    categories = []\n",
    "    for category, keywords in category_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            categories.append(category)\n",
    "    return categories\n",
    "\n",
    "# Apply categorization to \"Cleaned Caption\"\n",
    "df['categories'] = df['Cleaned Caption'].apply(categorize_post)\n",
    "\n",
    "# Remove rows with empty categories (previously uncategorized)\n",
    "df = df[df['categories'].apply(len) > 0]\n",
    "\n",
    "# Function to get top keywords using predefined list\n",
    "def get_top_keywords_predefined(text, keywords, top_n=5):\n",
    "    text = str(text).lower()\n",
    "    found_keywords = [word for word in keywords if word in text]\n",
    "    return found_keywords[:top_n]\n",
    "\n",
    "# Function to get top keywords using TF-IDF\n",
    "def get_top_keywords_tfidf(tfidf_vector, feature_names, top_n=5):\n",
    "    coo_matrix = tfidf_vector.tocoo()\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    sorted_items = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    sorted_items = sorted_items[:top_n]\n",
    "    return [feature_names[idx] for idx, _ in sorted_items]\n",
    "\n",
    "# Choose method: 'predefined' or 'tfidf'\n",
    "method = 'predefined'  # Change this to 'tfidf' if you want to use TF-IDF\n",
    "\n",
    "if method == 'predefined':\n",
    "    df['top_keywords'] = df['Cleaned Caption'].apply(lambda x: get_top_keywords_predefined(x, all_keywords))\n",
    "else:  # TF-IDF method\n",
    "    tfidf = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf.fit_transform(df['Cleaned Caption'].fillna(''))\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    df['top_keywords'] = [get_top_keywords_tfidf(tfidf_matrix[i], feature_names) for i in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "# Calculate category distribution\n",
    "category_distribution = df['categories'].explode().value_counts(normalize=True)\n",
    "print(\"Category Distribution:\")\n",
    "print(category_distribution)\n",
    "\n",
    "# Print top keywords for the first few rows\n",
    "print(\"\\nSample Top Keywords:\")\n",
    "print(df['top_keywords'].head())\n",
    "\n",
    "# Save the results\n",
    "output_path = os.path.join(file_path, output_file)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\nKeyword analysis complete. Results saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
